{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/homosapiens/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.49, 'neu': 0.277, 'pos': 0.234, 'compound': -0.5726}\n"
     ]
    }
   ],
   "source": [
    "# Create a SentimentIntensityAnalyzer object\n",
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Example text to analyze\n",
    "text = \"Nobel is so damn proud and dumb!\"\n",
    "\n",
    "# Get the sentiment scores\n",
    "sentiment_scores = sid_obj.polarity_scores(text)\n",
    "\n",
    "# Print the sentiment scores\n",
    "print(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import getpass\n",
    "import pandas as pd\n",
    "\n",
    "api_key = getpass.getpass('Please enter your YouTube API key : ')\n",
    "\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "video_id = \"1DgM_CEOKVE\"\n",
    "\n",
    "\n",
    "# Function to get replies for a specific comment\n",
    "def get_replies(youtube, parent_id, video_id):  # Added video_id as an argument\n",
    "    replies = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        reply_request = youtube.comments().list(\n",
    "            part=\"snippet\",\n",
    "            parentId=parent_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        reply_response = reply_request.execute()\n",
    "\n",
    "        for item in reply_response['items']:\n",
    "            comment = item['snippet']\n",
    "            replies.append({\n",
    "                'Timestamp': comment['publishedAt'],\n",
    "                'Username': comment['authorDisplayName'],\n",
    "                'VideoID': video_id,\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Date': comment['updatedAt'] if 'updatedAt' in comment else comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        next_page_token = reply_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return replies\n",
    "\n",
    "# Function to get all comments (including replies) for a single video\n",
    "def get_comments_for_video(youtube, video_id):\n",
    "    all_comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        comment_request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            pageToken=next_page_token,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100\n",
    "        )\n",
    "        comment_response = comment_request.execute()\n",
    "\n",
    "        for item in comment_response['items']:\n",
    "            top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "            all_comments.append({\n",
    "                'Timestamp': top_comment['publishedAt'],\n",
    "                'Username': top_comment['authorDisplayName'],\n",
    "                'VideoID': video_id,  # Directly using video_id from function parameter\n",
    "                'Comment': top_comment['textDisplay'],\n",
    "                'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
    "            })\n",
    "\n",
    "            # Fetch replies if there are any\n",
    "            if item['snippet']['totalReplyCount'] > 0:\n",
    "                all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_id))\n",
    "\n",
    "        next_page_token = comment_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "\n",
    "video_comments = get_comments_for_video(youtube, video_id)\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "comments_df = pd.DataFrame(video_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = comments_df[\"Comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neg         0.119568\n",
       "neu         0.753562\n",
       "pos         0.125315\n",
       "compound   -0.019077\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer  \n",
    "\n",
    "# Extract the comments into a variable  \n",
    "comments = comments_df[\"Comment\"].tolist()  # Convert to a list if it's a Series  \n",
    "\n",
    "# Create a SentimentIntensityAnalyzer object  \n",
    "sid_obj = SentimentIntensityAnalyzer()  \n",
    "\n",
    "# Function to get sentiment scores  \n",
    "def get_sentiment(comment):  \n",
    "    return sid_obj.polarity_scores(comment)  \n",
    "\n",
    "# Apply the function to the comments  \n",
    "sentiment_scores = [get_sentiment(comment) for comment in comments]  \n",
    "\n",
    "# Convert sentiment scores to a DataFrame  \n",
    "sentiment_df = pd.DataFrame(sentiment_scores)  \n",
    "\n",
    "# Calculate overall mean sentiment scores  \n",
    "overall_sentiment = sentiment_df.mean()  \n",
    "\n",
    "overall_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment  Percentage\n",
      "0  Negative       11.96\n",
      "1   Neutral       75.36\n",
      "2  Positive       12.53\n"
     ]
    }
   ],
   "source": [
    "# Determine the sentiment label and percentage\n",
    "sentiment_label = ['Negative', 'Neutral', 'Positive']\n",
    "sentiment_percentage = [\n",
    "    round(overall_sentiment['neg'] * 100, 2),\n",
    "    round(overall_sentiment['neu'] * 100, 2),\n",
    "    round(overall_sentiment['pos'] * 100, 2)\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with the sentiment label and percentage\n",
    "sentiment_result = pd.DataFrame({\n",
    "    'Sentiment': sentiment_label,\n",
    "    'Percentage': sentiment_percentage\n",
    "})\n",
    "\n",
    "print(sentiment_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
